{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: As in \"ONLINE BATCH SELECTION FOR FASTER TRAINING OF NEURAL NETWORKS\", \n",
    "# have one pass over the whole training set to calculate the loss of each sample, (usin any method, prioritzed loss or validated)\n",
    "# and upon sampling and training, recalculate the new loss that would be induced, but don't apply it\n",
    "\n",
    "# seems to be difficult for validated: one would need to sample a validation batch, eval on val. batch, train,\n",
    "# eval. on val batch and undo the changes. So it would require three forward passes and one backward pass\n",
    "# For prioritized it would be one forward and backward pass to get the precise gradient for every sample in the batch\n",
    "\n",
    "# TODO: implement the upper bound of the gradient norm as in Katharopoulos et al \"\"\n",
    "\n",
    "import torch\n",
    "import skorch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from exp_rep import PrioritizedReplayBuffer, ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, writer, test_loader, sampling_procedure,\n",
    "          buffer, train_batch_size, val_batch_size, num_updates, total_updates, sampling_beta,\n",
    "          improvement_mean, sampling_running_avg, sampling_anneal_beta, total_epochs, eval_uniform):\n",
    "    model.train()\n",
    "    remove_samples_from_buffer = (sampling_procedure == \"uniform\")\n",
    "        \n",
    "    start_beta = sampling_beta\n",
    "    for batch_idx in range(num_updates):\n",
    "        with torch.no_grad():\n",
    "            for test_data, test_target in test_loader:\n",
    "                output = model(test_data)\n",
    "                test_loss = F.nll_loss(output, test_target, reduction='sum').item() # sum up batch loss\n",
    "                writer.add_scalar(\"Test loss\", test_loss, global_step = total_updates)\n",
    "                break\n",
    "        # Get an estimate of the current train loss to track performance:\n",
    "        #with torch.no_grad():\n",
    "        #    for test_data, test_target in train_loader:\n",
    "        #        output = model(test_data)\n",
    "        #        test_loss = F.nll_loss(output, test_target, reduction='sum').item() # sum up batch loss\n",
    "        #        break\n",
    "        \n",
    "        if len(buffer) < train_batch_size:\n",
    "            train_batch_size = len(buffer)\n",
    "        # Anneal beta from original value to 1 over all epochs\n",
    "        sampling_beta = start_beta + (batch_idx / (total_epochs * num_updates)) * (1 - start_beta)\n",
    "        samples = buffer.sample(train_batch_size, beta=sampling_beta,\n",
    "                                remove_samples_from_buffer=remove_samples_from_buffer)\n",
    "        data = torch.from_numpy(samples[0]).float()\n",
    "        data = data.view((train_batch_size, 1, 28, 28))\n",
    "        target = torch.from_numpy(samples[2]).long()\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target, reduction='none')\n",
    "        if sampling_procedure != \"uniform\":\n",
    "            weights = torch.from_numpy(samples[-2]).float()\n",
    "            idxes = samples[-1]\n",
    "            loss_per_sample = loss.detach()\n",
    "            loss *= weights\n",
    "            if sampling_procedure == \"prioritized\":\n",
    "                buffer.update_priorities(idxes, loss_per_sample)\n",
    "            \n",
    "\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if sampling_procedure == \"validated_prioritized\" or eval_uniform:\n",
    "            with torch.no_grad():\n",
    "                # Calculate improvement after training:\n",
    "                updated_test_output = model(test_data)\n",
    "                updated_test_loss = F.nll_loss(updated_test_output, test_target, reduction='sum').item()\n",
    "                improvement = (test_loss - updated_test_loss)\n",
    "                writer.add_scalar(\"Validated Improvement\", improvement, global_step=total_updates)\n",
    "\n",
    "                \n",
    "                linear_boosted_clip_func = lambda x: np.clip(x + (1 if x > 0 else 0), 0.1, None)\n",
    "                relative_improvement_func = lambda x: (x - improvement_mean) / (improvement_mean if improvement_mean != 0 else 1)\n",
    "                sigmoid_func = lambda x: 1 / (0.1 + np.e ** (0.5 + -np.clip(x, -10, 10)))\n",
    "                \n",
    "                    \n",
    "                linear_boosted = linear_boosted_clip_func(improvement)\n",
    "                sigmoid_improvement = sigmoid_func(improvement)\n",
    "                relative_improvement = relative_improvement_func(improvement)\n",
    "                boosted_relative = linear_boosted_clip_func(relative_improvement)\n",
    "                sigmoid_relative = sigmoid_func(relative_improvement)\n",
    "                \n",
    "                writer.add_scalar(\"Validated Linear boosted Improvement\", linear_boosted, global_step=total_updates)\n",
    "                writer.add_scalar(\"Validated Sigmoid Improvement\", sigmoid_improvement, global_step=total_updates)                \n",
    "                writer.add_scalar(\"Validated Relative Improvement\", relative_improvement, global_step=total_updates)\n",
    "                writer.add_scalar(\"Validated Relative Sigmoid Improvement\", sigmoid_relative, global_step=total_updates)                \n",
    "                writer.add_scalar(\"Validated Relative Linear Boosted improvement\", boosted_relative, global_step=total_updates)\n",
    "                writer.add_scalar(\"Running Mean of Validated Improvement\", improvement_mean, global_step=total_updates)\n",
    "                # update running mean:\n",
    "                improvement_mean = improvement_mean * 0.99 + 0.01 * improvement\n",
    "                                              \n",
    "                improvement = sigmoid_relative\n",
    "\n",
    "            if sampling_procedure == \"validated_prioritized\":\n",
    "                prioritization_weights = np.ones_like(idxes) * improvement\n",
    "                buffer.update_priorities(idxes, prioritization_weights, running_avg=sampling_running_avg)\n",
    "       \n",
    "        \n",
    "        total_updates += 1\n",
    "        writer.add_scalar(\"Train loss\", loss.detach(), global_step=total_updates)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        if 1 == 2 and batch_idx != \"uniform\": #TODO: enable every fifth of training time\n",
    "            writer.add_histogram(\"Sampling distribution counts\", buffer.counts, global_step=epoch, bins='tensorflow', max_bins=None)\n",
    "            for i in range(60000):\n",
    "                weight_values[i] = buffer._it_sum[i]\n",
    "            writer.add_histogram(\"Sampling distribution Sum Tree Content\", weight_values, global_step=epoch, bins='tensorflow', max_bins=None)\n",
    "\n",
    "        \n",
    "                \n",
    "    return total_updates, improvement_mean\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.434606\n",
      "Train Epoch: 1 [80/60000 (0%)]\tLoss: 1.993711\n",
      "Train Epoch: 1 [160/60000 (0%)]\tLoss: 1.938647\n",
      "Train Epoch: 1 [240/60000 (0%)]\tLoss: 1.773226\n",
      "Train Epoch: 1 [320/60000 (1%)]\tLoss: 1.760959\n",
      "Train Epoch: 1 [400/60000 (1%)]\tLoss: 1.742299\n",
      "Train Epoch: 1 [480/60000 (1%)]\tLoss: 1.560375\n",
      "Train Epoch: 1 [560/60000 (1%)]\tLoss: 1.535210\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.572218\n",
      "Train Epoch: 1 [720/60000 (1%)]\tLoss: 1.014835\n",
      "Train Epoch: 1 [800/60000 (1%)]\tLoss: 1.165351\n",
      "Train Epoch: 1 [880/60000 (1%)]\tLoss: 0.455093\n",
      "Train Epoch: 1 [960/60000 (2%)]\tLoss: 0.323049\n",
      "Train Epoch: 1 [1040/60000 (2%)]\tLoss: 0.419058\n",
      "Train Epoch: 1 [1120/60000 (2%)]\tLoss: 0.243880\n",
      "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 0.273662\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.204712\n",
      "Train Epoch: 1 [1360/60000 (2%)]\tLoss: 0.244095\n",
      "Train Epoch: 1 [1440/60000 (2%)]\tLoss: 0.253311\n",
      "Train Epoch: 1 [1520/60000 (3%)]\tLoss: 0.194241\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.125543\n",
      "Train Epoch: 1 [1680/60000 (3%)]\tLoss: 0.155016\n",
      "Train Epoch: 1 [1760/60000 (3%)]\tLoss: 0.181397\n",
      "Train Epoch: 1 [1840/60000 (3%)]\tLoss: 0.111009\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.158579\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.139224\n",
      "Train Epoch: 1 [2080/60000 (3%)]\tLoss: 0.099371\n",
      "Train Epoch: 1 [2160/60000 (4%)]\tLoss: 0.080069\n",
      "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 0.071808\n",
      "Train Epoch: 1 [2320/60000 (4%)]\tLoss: 0.118725\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.061082\n",
      "Train Epoch: 1 [2480/60000 (4%)]\tLoss: 0.063257\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.070615\n",
      "Train Epoch: 1 [2640/60000 (4%)]\tLoss: 0.031361\n",
      "Train Epoch: 1 [2720/60000 (5%)]\tLoss: 0.026134\n",
      "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 0.058737\n",
      "Train Epoch: 1 [2880/60000 (5%)]\tLoss: 0.021625\n",
      "Train Epoch: 1 [2960/60000 (5%)]\tLoss: 0.023526\n",
      "Train Epoch: 1 [3040/60000 (5%)]\tLoss: 0.026564\n",
      "Train Epoch: 1 [3120/60000 (5%)]\tLoss: 0.033852\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.038380\n",
      "Train Epoch: 1 [3280/60000 (5%)]\tLoss: 0.046507\n",
      "Train Epoch: 1 [3360/60000 (6%)]\tLoss: 0.022978\n",
      "Train Epoch: 1 [3440/60000 (6%)]\tLoss: 0.013050\n",
      "Train Epoch: 1 [3520/60000 (6%)]\tLoss: 0.037424\n",
      "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 0.034589\n",
      "Train Epoch: 1 [3680/60000 (6%)]\tLoss: 0.031799\n",
      "Train Epoch: 1 [3760/60000 (6%)]\tLoss: 0.023955\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.019424\n",
      "Train Epoch: 1 [3920/60000 (7%)]\tLoss: 0.028536\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.036135\n",
      "Train Epoch: 1 [4080/60000 (7%)]\tLoss: 0.038587\n",
      "Train Epoch: 1 [4160/60000 (7%)]\tLoss: 0.024979\n",
      "Train Epoch: 1 [4240/60000 (7%)]\tLoss: 0.022067\n",
      "Train Epoch: 1 [4320/60000 (7%)]\tLoss: 0.015929\n",
      "Train Epoch: 1 [4400/60000 (7%)]\tLoss: 0.021124\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.022128\n",
      "Train Epoch: 1 [4560/60000 (8%)]\tLoss: 0.023800\n",
      "Train Epoch: 1 [4640/60000 (8%)]\tLoss: 0.020288\n",
      "Train Epoch: 1 [4720/60000 (8%)]\tLoss: 0.027163\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.025007\n",
      "Train Epoch: 1 [4880/60000 (8%)]\tLoss: 0.020067\n",
      "Train Epoch: 1 [4960/60000 (8%)]\tLoss: 0.013023\n",
      "Train Epoch: 1 [5040/60000 (8%)]\tLoss: 0.016984\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.034365\n",
      "Train Epoch: 1 [5200/60000 (9%)]\tLoss: 0.017420\n",
      "Train Epoch: 1 [5280/60000 (9%)]\tLoss: 0.023201\n",
      "Train Epoch: 1 [5360/60000 (9%)]\tLoss: 0.023045\n",
      "Train Epoch: 1 [5440/60000 (9%)]\tLoss: 0.018265\n",
      "Train Epoch: 1 [5520/60000 (9%)]\tLoss: 0.024145\n",
      "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 0.014525\n",
      "Train Epoch: 1 [5680/60000 (9%)]\tLoss: 0.017311\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.020231\n",
      "Train Epoch: 1 [5840/60000 (10%)]\tLoss: 0.021913\n",
      "Train Epoch: 1 [5920/60000 (10%)]\tLoss: 0.020825\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.026934\n",
      "Train Epoch: 1 [6080/60000 (10%)]\tLoss: 0.018747\n",
      "Train Epoch: 1 [6160/60000 (10%)]\tLoss: 0.014030\n",
      "Train Epoch: 1 [6240/60000 (10%)]\tLoss: 0.021239\n",
      "Train Epoch: 1 [6320/60000 (11%)]\tLoss: 0.009231\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.017990\n",
      "Train Epoch: 1 [6480/60000 (11%)]\tLoss: 0.011556\n",
      "Train Epoch: 1 [6560/60000 (11%)]\tLoss: 0.016873\n",
      "Train Epoch: 1 [6640/60000 (11%)]\tLoss: 0.009875\n",
      "Train Epoch: 1 [6720/60000 (11%)]\tLoss: 0.016991\n",
      "Train Epoch: 1 [6800/60000 (11%)]\tLoss: 0.016062\n",
      "Train Epoch: 1 [6880/60000 (11%)]\tLoss: 0.010996\n",
      "Train Epoch: 1 [6960/60000 (12%)]\tLoss: 0.012950\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.013886\n",
      "Train Epoch: 1 [7120/60000 (12%)]\tLoss: 0.009260\n",
      "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 0.014227\n",
      "Train Epoch: 1 [7280/60000 (12%)]\tLoss: 0.008753\n",
      "Train Epoch: 1 [7360/60000 (12%)]\tLoss: 0.006457\n",
      "Train Epoch: 1 [7440/60000 (12%)]\tLoss: 0.011333\n",
      "Train Epoch: 1 [7520/60000 (13%)]\tLoss: 0.005614\n",
      "Train Epoch: 1 [7600/60000 (13%)]\tLoss: 0.006767\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.007673\n",
      "Train Epoch: 1 [7760/60000 (13%)]\tLoss: 0.027239\n",
      "Train Epoch: 1 [7840/60000 (13%)]\tLoss: 0.013521\n",
      "Train Epoch: 1 [7920/60000 (13%)]\tLoss: 0.012074\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.010158\n",
      "Train Epoch: 1 [8080/60000 (13%)]\tLoss: 0.015081\n",
      "Train Epoch: 1 [8160/60000 (14%)]\tLoss: 0.008149\n",
      "Train Epoch: 1 [8240/60000 (14%)]\tLoss: 0.012820\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.005990\n",
      "Train Epoch: 1 [8400/60000 (14%)]\tLoss: 0.006431\n",
      "Train Epoch: 1 [8480/60000 (14%)]\tLoss: 0.006108\n",
      "Train Epoch: 1 [8560/60000 (14%)]\tLoss: 0.016367\n",
      "Train Epoch: 1 [8640/60000 (14%)]\tLoss: 0.005105\n",
      "Train Epoch: 1 [8720/60000 (15%)]\tLoss: 0.009876\n",
      "Train Epoch: 1 [8800/60000 (15%)]\tLoss: 0.009564\n",
      "Train Epoch: 1 [8880/60000 (15%)]\tLoss: 0.004887\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.013437\n",
      "Train Epoch: 1 [9040/60000 (15%)]\tLoss: 0.017479\n",
      "Train Epoch: 1 [9120/60000 (15%)]\tLoss: 0.007005\n",
      "Train Epoch: 1 [9200/60000 (15%)]\tLoss: 0.006624\n",
      "Train Epoch: 1 [9280/60000 (15%)]\tLoss: 0.009817\n",
      "Train Epoch: 1 [9360/60000 (16%)]\tLoss: 0.022934\n",
      "Train Epoch: 1 [9440/60000 (16%)]\tLoss: 0.003741\n",
      "Train Epoch: 1 [9520/60000 (16%)]\tLoss: 0.004866\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.007319\n",
      "Train Epoch: 1 [9680/60000 (16%)]\tLoss: 0.008090\n",
      "Train Epoch: 1 [9760/60000 (16%)]\tLoss: 0.011864\n",
      "Train Epoch: 1 [9840/60000 (16%)]\tLoss: 0.009351\n",
      "Train Epoch: 1 [9920/60000 (17%)]\tLoss: 0.008781\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.004086\n",
      "Train Epoch: 1 [10080/60000 (17%)]\tLoss: 0.008291\n",
      "Train Epoch: 1 [10160/60000 (17%)]\tLoss: 0.004585\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.012695\n",
      "Train Epoch: 1 [10320/60000 (17%)]\tLoss: 0.008366\n",
      "Train Epoch: 1 [10400/60000 (17%)]\tLoss: 0.005872\n",
      "Train Epoch: 1 [10480/60000 (17%)]\tLoss: 0.004302\n",
      "Train Epoch: 1 [10560/60000 (18%)]\tLoss: 0.004842\n",
      "Train Epoch: 1 [10640/60000 (18%)]\tLoss: 0.004008\n",
      "Train Epoch: 1 [10720/60000 (18%)]\tLoss: 0.011795\n",
      "Train Epoch: 1 [10800/60000 (18%)]\tLoss: 0.003160\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.004954\n",
      "Train Epoch: 1 [10960/60000 (18%)]\tLoss: 0.010566\n",
      "Train Epoch: 1 [11040/60000 (18%)]\tLoss: 0.012947\n",
      "Train Epoch: 1 [11120/60000 (19%)]\tLoss: 0.009177\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.003587\n",
      "Train Epoch: 1 [11280/60000 (19%)]\tLoss: 0.002163\n",
      "Train Epoch: 1 [11360/60000 (19%)]\tLoss: 0.003146\n",
      "Train Epoch: 1 [11440/60000 (19%)]\tLoss: 0.008352\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.010411\n",
      "Train Epoch: 1 [11600/60000 (19%)]\tLoss: 0.003900\n",
      "Train Epoch: 1 [11680/60000 (19%)]\tLoss: 0.007649\n",
      "Train Epoch: 1 [11760/60000 (20%)]\tLoss: 0.002461\n",
      "Train Epoch: 1 [11840/60000 (20%)]\tLoss: 0.004020\n",
      "Train Epoch: 1 [11920/60000 (20%)]\tLoss: 0.002723\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.004947\n",
      "Train Epoch: 1 [12080/60000 (20%)]\tLoss: 0.006694\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.011656\n",
      "Train Epoch: 1 [12240/60000 (20%)]\tLoss: 0.005958\n",
      "Train Epoch: 1 [12320/60000 (21%)]\tLoss: 0.003784\n",
      "Train Epoch: 1 [12400/60000 (21%)]\tLoss: 0.003262\n",
      "Train Epoch: 1 [12480/60000 (21%)]\tLoss: 0.005287\n",
      "Train Epoch: 1 [12560/60000 (21%)]\tLoss: 0.003614\n",
      "Train Epoch: 1 [12640/60000 (21%)]\tLoss: 0.005634\n",
      "Train Epoch: 1 [12720/60000 (21%)]\tLoss: 0.007159\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.006874\n",
      "Train Epoch: 1 [12880/60000 (21%)]\tLoss: 0.003223\n",
      "Train Epoch: 1 [12960/60000 (22%)]\tLoss: 0.004844\n",
      "Train Epoch: 1 [13040/60000 (22%)]\tLoss: 0.008853\n",
      "Train Epoch: 1 [13120/60000 (22%)]\tLoss: 0.003517\n",
      "Train Epoch: 1 [13200/60000 (22%)]\tLoss: 0.006207\n",
      "Train Epoch: 1 [13280/60000 (22%)]\tLoss: 0.003125\n",
      "Train Epoch: 1 [13360/60000 (22%)]\tLoss: 0.006927\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.004443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [13520/60000 (23%)]\tLoss: 0.004138\n",
      "Train Epoch: 1 [13600/60000 (23%)]\tLoss: 0.005405\n",
      "Train Epoch: 1 [13680/60000 (23%)]\tLoss: 0.003843\n",
      "Train Epoch: 1 [13760/60000 (23%)]\tLoss: 0.008907\n",
      "Train Epoch: 1 [13840/60000 (23%)]\tLoss: 0.004915\n",
      "Train Epoch: 1 [13920/60000 (23%)]\tLoss: 0.005773\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-db11bcd1e848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m                           \u001b[0mval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimprovement_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                            \u001b[0msampling_running_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_anneal_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                                             eval_uniform=eval_uniform)\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-2692de8b5bb2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, writer, test_loader, sampling_procedure, buffer, train_batch_size, val_batch_size, num_updates, total_updates, sampling_beta, improvement_mean, sampling_running_avg, sampling_anneal_beta, total_epochs, eval_uniform)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msampling_procedure\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"validated_prioritized\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0meval_uniform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "save_model = False\n",
    "use_cuda = torch.cuda.is_available()\n",
    "eval_uniform = True\n",
    "sampling_procedure = \"prioritized\" # \"uniform\" \"prioritized\", \"validated_prioritized\"\n",
    "sampling_alpha = 1\n",
    "sampling_beta = 0.4\n",
    "sampling_max_priority = 2\n",
    "sampling_running_avg = 0.5\n",
    "sampling_anneal_beta = False\n",
    "train_batch_size = 8\n",
    "val_batch_size = 128\n",
    "lr = 0.005\n",
    "improvement_mean = 0\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=train_batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=val_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "num_train_samples = 60000\n",
    "if sampling_procedure == \"uniform\":\n",
    "    buffer = ReplayBuffer(num_train_samples)\n",
    "else:\n",
    "    buffer = PrioritizedReplayBuffer(num_train_samples, sampling_alpha, max_priority=sampling_max_priority)\n",
    "\n",
    "for batch in train_loader:\n",
    "    for idx in range(len(batch[0])):\n",
    "        buffer.add(batch[0][idx], None, batch[1][idx], None, False)\n",
    "    \n",
    "total_updates = 0\n",
    "num_updates = num_train_samples // train_batch_size \n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "writer = SummaryWriter()\n",
    "#writer.add_graph(model)\n",
    "\n",
    "weight_values = np.zeros(60000)\n",
    "total_epochs = 3\n",
    "for epoch in range(1, total_epochs + 1):\n",
    "    total_updates, improvement_mean = train(model, device, train_loader, optimizer, epoch, writer,\n",
    "                          test_loader, sampling_procedure, buffer, train_batch_size,\n",
    "                          val_batch_size, num_updates, total_updates, sampling_beta, improvement_mean,\n",
    "                                           sampling_running_avg, sampling_anneal_beta, total_epochs, \n",
    "                                            eval_uniform=eval_uniform)\n",
    "    test(model, device, test_loader)\n",
    "    \n",
    "    if sampling_procedure != \"uniform\":\n",
    "        writer.add_histogram(\"Sampling distribution counts\", buffer.counts, global_step=epoch, bins='tensorflow', max_bins=None)\n",
    "        for i in range(60000):\n",
    "            weight_values[i] = buffer._it_sum[i]\n",
    "        writer.add_histogram(\"Sampling distribution\", weight_values, global_step=epoch, bins='tensorflow', max_bins=None)\n",
    "\n",
    "    \n",
    "if (save_model):\n",
    "    torch.save(model.state_dict(),\"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    for i in range(len(batch[0])):\n",
    "        print(batch[0][i], batch[1][i])\n",
    "    break\n",
    "\n",
    "#for idx, batch in enumerate(train_loader):\n",
    "#    for idx in range(len(batch[0])):\n",
    "#        buffer.add(batch[0][idx], None, batch[1][idx], None, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = buffer.sample(train_batch_size)\n",
    "imgs = torch.from_numpy(samples[0])\n",
    "labels = torch.from_numpy(samples[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
